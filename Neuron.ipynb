{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function `dot` which calculates the dot product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot x y = sum $ zipWith(*) x y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `zipWith` to multiply together the components in each vector and then we use sum to add all of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot [1,2] [3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use that to define a single neuron. A neuro has 3 properties that are interesting:\n",
    "* It has input weights which they are numbers that scales the inputs\n",
    "* It has a bias which is a single number that it's added to the input singals\n",
    "* It has an activation function which controls what the output of the neuron is like\n",
    "\n",
    "A neuron itself in Machine Learning terms is a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron weights bias activation inputs = let b = dot weights inputs + bias in \n",
    "                                        activation b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually use a neuron we need to pick an activation function. There are 3 commons ones:\n",
    "* **reLU**: If the input < 0 then output 0, otherwise output the input. \n",
    "* **Sigmoid**: If the input is very low then 0 if the input is very high then 1 and between the two of them there is a smooth S shape curve from 0 to 1 when the input is close to 0.\n",
    "* **Tanh**: It's similar to the sigmoid but it has a different shape near the top and the bottom of the S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reLU x = if x < 0 then 0 else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function we now have everything we need to actually use the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mineneuron = neuron [1] 0 reLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's everything we should do to define a neuron. We now have a neuron where the `weight` is 1, the `bias` is 0 and the `activation function` is reLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mineneuron [0]\n",
    "mineneuron [1]\n",
    "mineneuron [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this weight and bias the neuron does not do very much so let's make a more interesting neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron2 = neuron [2] 1 reLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we test our neuron with some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neuron2 [0]\n",
    "neuron2 [1]\n",
    "neuron2 [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron with a simple input is doing `output = weight * input + bias`. This is the same formula as the formula for an straight line `y = m * x + b`. So, our neuron is modelling a straigh line by changing the `weight` and the `bias` we change the line. Also the activation function means that we do not output just the line but instead we clamp values where `y < 0`. If we just do a graph `y = 2x + 1` then it would be a straigh line the goes both up and to the right but also down onto the left and the `activitation function` means that we stop the neuron going down onto the left. Withou the activation function could only ever describe a straigh line so we use activation functions to let the neuron descrive more complex shapes like curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiweightneuron = neuron [2,3,5,6] 0 reLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiweightneuron [1,2,3,4]\n",
    "multiweightneuron [10,20,30,40]\n",
    "multiweightneuron [-1,0,2,3]\n",
    "multiweightneuron [1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the last of those because all the inputs are `0` apart of one of them it is like we are selecting one of the `weights` to output by only passing a 1 for that input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiweightneuron [1,0,0,0]\n",
    "multiweightneuron [0,1,0,0]\n",
    "multiweightneuron [0,0,1,0]\n",
    "multiweightneuron [0,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `outputs` that we get are the `weight` values. this is because by setting the other inputs to 0 we make sure that all of the other weights get multiply into nothing so only the weight for the input we said one will be included in the `output`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell - haskell",
   "language": "haskell",
   "name": "ihaskell_haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
